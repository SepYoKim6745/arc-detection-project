{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5921757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65c4e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼명 정의\n",
    "column_names = ['timestamp', 'v_raw', 'c_raw', 'voltage', 'current', 'label']\n",
    "\n",
    "# 정상 데이터\n",
    "normal_dir = '../realtime/normal/'\n",
    "normal_files = glob.glob(os.path.join(normal_dir, '*.csv'))\n",
    "normal_dfs = [pd.read_csv(file, names=column_names, header=None) for file in normal_files]\n",
    "normal_data = pd.concat(normal_dfs, ignore_index=True)\n",
    "\n",
    "# 아크 데이터\n",
    "arc_dir = '../realtime/arc/'\n",
    "arc_files = glob.glob(os.path.join(arc_dir, '*.csv'))\n",
    "arc_dfs = [pd.read_csv(file, names=column_names, header=None) for file in arc_files]\n",
    "arc_data = pd.concat(arc_dfs, ignore_index=True)\n",
    "\n",
    "# 전체 병합 및 셔플\n",
    "train_df = pd.concat([normal_data, arc_data], ignore_index=True)\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64d9aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파생 피처\n",
    "def add_features(df):\n",
    "    df['voltage_diff'] = df['voltage'].diff().fillna(0).abs()\n",
    "    df['current_diff'] = df['current'].diff().fillna(0).abs()\n",
    "    df['voltage_ma'] = df['voltage'].rolling(5).mean().bfill()\n",
    "    df['current_ma'] = df['current'].rolling(5).mean().bfill()\n",
    "    df['power'] = df['voltage'] * df['current']\n",
    "    df['power_diff'] = df['power'].diff().fillna(0).abs()\n",
    "    return df\n",
    "\n",
    "df = add_features(train_df).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "735d0caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['voltage', 'current', 'voltage_diff', 'current_diff',\n",
    "            'voltage_ma', 'current_ma', 'power', 'power_diff']\n",
    "\n",
    "X_raw = df[features].values\n",
    "y_raw = df['label'].values.astype(int)\n",
    "\n",
    "# 정규화\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "935e595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시퀀스 생성\n",
    "def create_sequences(data, labels, seq_len=8):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_len):\n",
    "        X.append(data[i:i+seq_len])\n",
    "        y.append(labels[i+seq_len - 1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_len = 8\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_raw, seq_len)\n",
    "X_seq = X_seq.reshape((X_seq.shape[0], seq_len, len(features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "720fc160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습/검증 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_seq, y_seq, test_size=0.2, stratify=y_seq, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ff9d605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8718 - loss: 0.3873 - val_accuracy: 0.8854 - val_loss: 0.3590\n",
      "Epoch 2/50\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8866 - loss: 0.3482 - val_accuracy: 0.8854 - val_loss: 0.3432\n",
      "Epoch 3/50\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8884 - loss: 0.3370 - val_accuracy: 0.8852 - val_loss: 0.3263\n",
      "Epoch 4/50\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8859 - loss: 0.3325 - val_accuracy: 0.8960 - val_loss: 0.3209\n",
      "Epoch 5/50\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8929 - loss: 0.3153 - val_accuracy: 0.8962 - val_loss: 0.3111\n",
      "Epoch 6/50\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8912 - loss: 0.3156 - val_accuracy: 0.9002 - val_loss: 0.3037\n",
      "Epoch 7/50\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8964 - loss: 0.3075 - val_accuracy: 0.9041 - val_loss: 0.3013\n",
      "Epoch 8/50\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9011 - loss: 0.3044 - val_accuracy: 0.9012 - val_loss: 0.3016\n",
      "Epoch 9/50\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8943 - loss: 0.3105 - val_accuracy: 0.9035 - val_loss: 0.2972\n",
      "Epoch 10/50\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9035 - loss: 0.2943 - val_accuracy: 0.9031 - val_loss: 0.2957\n",
      "Epoch 11/50\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9014 - loss: 0.2966 - val_accuracy: 0.9037 - val_loss: 0.2950\n",
      "Epoch 12/50\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9019 - loss: 0.2948 - val_accuracy: 0.9057 - val_loss: 0.2946\n",
      "Epoch 13/50\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9037 - loss: 0.2929 - val_accuracy: 0.9049 - val_loss: 0.2932\n",
      "Epoch 14/50\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9039 - loss: 0.2940 - val_accuracy: 0.9061 - val_loss: 0.2894\n",
      "Epoch 15/50\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9046 - loss: 0.2883 - val_accuracy: 0.9083 - val_loss: 0.2884\n",
      "Epoch 16/50\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9067 - loss: 0.2849 - val_accuracy: 0.9063 - val_loss: 0.2890\n",
      "Epoch 17/50\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9080 - loss: 0.2831 - val_accuracy: 0.9091 - val_loss: 0.2846\n",
      "Epoch 18/50\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9054 - loss: 0.2871 - val_accuracy: 0.9081 - val_loss: 0.2856\n",
      "Epoch 19/50\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9079 - loss: 0.2792 - val_accuracy: 0.9077 - val_loss: 0.2876\n",
      "Epoch 20/50\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9091 - loss: 0.2769 - val_accuracy: 0.9079 - val_loss: 0.2889\n",
      "Epoch 21/50\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9143 - loss: 0.2639 - val_accuracy: 0.9083 - val_loss: 0.2879\n",
      "Epoch 22/50\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9113 - loss: 0.2700 - val_accuracy: 0.9095 - val_loss: 0.2862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c99814bd70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = Input(shape=(seq_len, len(features)))\n",
    "x = Conv1D(64, 3, activation='relu', padding='same')(inputs)\n",
    "x = MaxPooling1D(2)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Conv1D(128, 3, activation='relu', padding='same')(x)\n",
    "x = MaxPooling1D(2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 학습\n",
    "early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=64, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f74fee42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./model/scaler_cnn.joblib']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 및 스케일러 저장\n",
    "model.save('./model/cnn1d_model.h5')\n",
    "joblib.dump(scaler, './model/scaler_cnn.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f713c951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\NGNLAB~1\\AppData\\Local\\Temp\\tmpus32aa8u\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\NGNLAB~1\\AppData\\Local\\Temp\\tmpus32aa8u\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\NGNLAB~1\\AppData\\Local\\Temp\\tmpus32aa8u'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 8, 8), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1965388570192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1965388566928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1965388568272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1965388567888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1965388563280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1965388569040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1965388565776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1965388569616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "✅ 변환 성공: TFLite 저장 완료\n"
     ]
    }
   ],
   "source": [
    "# ✅ TFLite 변환\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "model = tf.keras.models.load_model('./model/cnn1d_model.h5')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# 저장\n",
    "with open('./model/cnn1d_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"✅ 변환 성공: TFLite 저장 완료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
